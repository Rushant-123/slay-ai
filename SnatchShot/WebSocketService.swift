//
//  WebSocketService.swift
//  SnatchShot
//
//  Created by Rushant on 15/09/25.
//

import Foundation
import Combine
import UIKit
import AVFoundation
import SuperwallKit

// MARK: - iOS WebSocket Architecture Notes
//
// iOS WebSocket Limitations:
// - Maximum message size: 1MB (system-imposed by WebKit/iOS)
// - Cannot be increased via code - this is a system-level limitation
// - Large messages (>1MB) will fail with "Message too long" error
//
// Our Solution:
// - WebSocket: Real-time progress updates, analysis data, status events
// - HTTP: Large image data (works around WebSocket size limits)
// - Binary Image Handling: Server sends compressed JPEG buffers, we convert to base64
// - This is the recommended iOS WebSocket pattern for media-heavy apps
//
// New Binary Image Handling (Server Update):
// - Server sends compressed JPEG data directly as binary WebSocket messages
// - iOS receives binary data and converts to base64 for compatibility
// - Binary messages are stored as "pending" until matched with metadata events
// - Text events provide titles/metadata, binary events provide image data
// - This avoids the 1MB limit while maintaining real-time delivery
//
// Expected Behavior:
// - WebSocket connects successfully and receives all progress events
// - Binary image data arrives separately from text metadata
// - "Message too long" errors are expected and handled gracefully
// - HTTP fallback provides complete images when WebSocket can't
// - Users get smooth progress updates + final images seamlessly
// - Images are properly compressed and ready for display

// MARK: - WebSocket Event Types
enum WebSocketEventType: String, Decodable {
    case connection_established
    case processing_started
    case analysis_started
    case content_analysis_complete
    case photo_analysis_complete
    case image_generation_started
    case individual_image_started
    case individual_image_completed
    case individual_image_error
    case image_generation_completed
    case image_generation_error
    case processing_completed
    case processing_error
    case usage_stats
    case plan_limit_reached
}

// MARK: - WebSocket Event Models
struct WebSocketEvent: Decodable {
    let type: WebSocketEventType
    let requestId: String?
    let message: String?
    let timestamp: String
    let imageData: String? // base64 for individual_image_completed
    let imageIndex: Int? // for individual_image_started/completed/error
    let totalImages: Int? // for individual_image_started/completed/error
    let title: String? // for individual_image_started/completed/error
    let error: String? // for error events
    let settings: CameraSettings? // for content_analysis_complete
    let analysis: WebSocketAnalysis? // for photo_analysis_complete
    // Usage stats fields
    let userId: String? // for usage_stats
    let currentUsage: Int? // for usage_stats
    let limit: Int? // for usage_stats and plan_limit_reached
    let plan: String? // for usage_stats and plan_limit_reached
    let resetDate: String? // for usage_stats and plan_limit_reached

    enum CodingKeys: String, CodingKey {
        case type, requestId, message, timestamp, imageData, imageIndex, totalImages, title, error, settings, analysis, userId, currentUsage, limit, plan, resetDate
    }

    init(from decoder: Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        type = try container.decode(WebSocketEventType.self, forKey: .type)
        requestId = try container.decodeIfPresent(String.self, forKey: .requestId)
        message = try container.decodeIfPresent(String.self, forKey: .message)
        timestamp = try container.decode(String.self, forKey: .timestamp)
        imageData = try container.decodeIfPresent(String.self, forKey: .imageData)
        imageIndex = try container.decodeIfPresent(Int.self, forKey: .imageIndex)
        totalImages = try container.decodeIfPresent(Int.self, forKey: .totalImages)
        title = try container.decodeIfPresent(String.self, forKey: .title)
        error = try container.decodeIfPresent(String.self, forKey: .error)

        // Decode nested objects conditionally
        settings = try container.decodeIfPresent(CameraSettings.self, forKey: .settings)
        analysis = try container.decodeIfPresent(WebSocketAnalysis.self, forKey: .analysis)

        // Decode usage stats fields
        userId = try container.decodeIfPresent(String.self, forKey: .userId)
        currentUsage = try container.decodeIfPresent(Int.self, forKey: .currentUsage)
        limit = try container.decodeIfPresent(Int.self, forKey: .limit)
        plan = try container.decodeIfPresent(String.self, forKey: .plan)
        resetDate = try container.decodeIfPresent(String.self, forKey: .resetDate)
    }
}

struct WebSocketAnalysis: Decodable {
    let photoImprovementSuggestions: [PhotoSuggestion]?

    private enum CodingKeys: String, CodingKey {
        case photoImprovementSuggestions = "photo_improvement_suggestions"
    }
}

struct CameraSettings: Decodable {
    let exposure_controls: ExposureControls?
    let white_balance_controls: WhiteBalanceControls?
    let image_processing: ImageProcessing?
    let advanced_modes: AdvancedModes?
    let professional_features: ProfessionalFeatures?
}

struct ExposureControls: Decodable {
    let exposure_mode: String?
    let iso: Double?
    let shutter_speed: Double?
    let aperture: Double?
    let ev_bias: Double?
    let focal_length: Double?
    let frame_rate: Double?
    let bracketing_mode: String?
}

struct WhiteBalanceControls: Decodable {
    let wb_preset: String?
    let white_balance_temperature: Double?
    let white_balance_tint: Double?
}

struct ImageProcessing: Decodable {
    let contrast: Double?
    let brightness: Double?
    let saturation: Double?
    let sharpness: Double?
    let current_filter: String?
}

struct AdvancedModes: Decodable {
    let night_mode: String?
    let night_mode_intensity: Double?
    let stabilization_mode: String?
    let burst_mode: String?
    let zoom_mode: String?
    let hdr_mode: Bool?
    let raw_capture: Bool?
}

struct ProfessionalFeatures: Decodable {
    let focus_peaking: Bool?
    let zebra_stripes: Bool?
    let level_indicator: Bool?
    let histogram: Bool?
    let audio_recording: Bool?
    let gps_tagging: Bool?
}

// MARK: - Enhanced Camera Settings for AI Recommendations

/// Comprehensive camera settings recommendation from AI analysis
struct AICameraRecommendation: Decodable {
    /// Overall confidence in these recommendations (0.0 to 1.0)
    let confidence: Float

    /// Human-readable explanation of why these settings were chosen
    let reasoning: String

    /// Detected scene type (affects which settings are prioritized)
    let sceneType: String // "portrait", "landscape", "low-light", "action", "macro", etc.

    /// Time in seconds for smooth transitions between settings
    let transitionDuration: TimeInterval

    /// Core exposure settings (most important - always include these)
    let exposure: AIExposureSettings?

    /// Focus and depth of field settings (critical for sharp images)
    let focus: AIFocusSettings?

    /// White balance and color temperature (essential for color accuracy)
    let whiteBalance: AIWhiteBalanceSettings?

    /// Image processing adjustments (enhance the final result)
    let processing: AIProcessingSettings?

    /// Special modes and features (situational optimizations)
    let special: AISpecialSettings?

    /// Safety bounds for the current device/camera
    let bounds: AISafetyBounds?

    private enum CodingKeys: String, CodingKey {
        case confidence, reasoning, sceneType = "scene_type"
        case transitionDuration = "transition_duration"
        case exposure, focus, whiteBalance = "white_balance"
        case processing, special, bounds
    }
}

/// Exposure triangle settings with AI recommendations
struct AIExposureSettings: Decodable {
    /// Exposure mode: "auto", "manual", "aperture_priority", "shutter_priority", "program"
    let mode: String?

    /// ISO sensitivity (will be clamped to device capabilities)
    let iso: Float?

    /// Shutter speed in seconds (e.g., 0.0167 for 1/60s)
    let shutterSpeed: Float?

    /// Aperture f-stop value
    let aperture: Float?

    /// Exposure compensation in EV (-2.0 to +2.0)
    let evBias: Float?

    private enum CodingKeys: String, CodingKey {
        case mode, iso, shutterSpeed = "shutter_speed", aperture, evBias = "ev_bias"
    }
}

/// Focus and depth of field settings
struct AIFocusSettings: Decodable {
    /// Focus mode: "auto", "manual", "continuous", "single"
    let mode: String?

    /// Manual focus distance (0.0 = closest, 1.0 = infinity)
    let distance: Float?

    /// Touch-to-focus coordinates (normalized 0.0-1.0)
    let pointOfInterest: [Float]? // [x, y] coordinates

    private enum CodingKeys: String, CodingKey {
        case mode, distance, pointOfInterest = "point_of_interest"
    }
}

/// White balance and color temperature
struct AIWhiteBalanceSettings: Decodable {
    /// Preset: "auto", "sunny", "cloudy", "shade", "tungsten", "fluorescent"
    let preset: String?

    /// Color temperature in Kelvin (2000-10000)
    let temperature: Float?

    /// Green/magenta tint adjustment (-50 to +50)
    let tint: Float?
}

/// Image processing and enhancement
struct AIProcessingSettings: Decodable {
    /// Brightness adjustment (-0.5 to 0.5)
    let brightness: Float?

    /// Contrast adjustment (0.5 to 2.0)
    let contrast: Float?

    /// Saturation adjustment (0.0 to 2.0)
    let saturation: Float?

    /// Sharpness adjustment (0.0 to 2.0)
    let sharpness: Float?

    /// Camera filter to apply (matches CameraFilter enum names)
    let filter: String?
}

/// Special camera modes and features
struct AISpecialSettings: Decodable {
    /// Night mode: "off", "auto", "on"
    let nightMode: String?

    /// Stabilization: "off", "on", "cinematic"
    let stabilization: String?

    /// Zoom factor (1.0 = no zoom, higher = more zoom)
    let zoom: Float?

    /// Flash mode: "off", "on", "auto"
    let flash: String?

    /// High Dynamic Range mode
    let hdr: Bool?

    /// Burst mode: "off", "low", "medium", "high"
    let burstMode: String?

    private enum CodingKeys: String, CodingKey {
        case nightMode = "night_mode", stabilization, zoom, flash, hdr, burstMode = "burst_mode"
    }
}

/// Safety bounds for the current device capabilities
struct AISafetyBounds: Decodable {
    /// ISO range
    let isoMin: Float?
    let isoMax: Float?

    /// Shutter speed range in seconds
    let shutterMin: Float?
    let shutterMax: Float?

    /// Aperture range
    let apertureMin: Float?
    let apertureMax: Float?

    /// Zoom range
    let zoomMin: Float?
    let zoomMax: Float?

    private enum CodingKeys: String, CodingKey {
        case isoMin = "iso_min", isoMax = "iso_max"
        case shutterMin = "shutter_min", shutterMax = "shutter_max"
        case apertureMin = "aperture_min", apertureMax = "aperture_max"
        case zoomMin = "zoom_min", zoomMax = "zoom_max"
    }
}

// MARK: - BACKEND JSON SPECIFICATION

/*
================================================================================
ðŸ“‹ BACKEND JSON SPECIFICATION FOR AI CAMERA SETTINGS
================================================================================

Copy this entire specification to Slack for backend developers.

The JSON structure below is what the backend MUST send via WebSocket for AI camera settings.
All fields are OPTIONAL - if AI misses any setting, we fall back to device defaults.

================================================================================

REQUIRED JSON STRUCTURE:
{
  "type": "camera_settings",  // Required: identifies this as camera settings message
  "confidence": 0.85,         // Required: AI confidence level (0.0 to 1.0)
  "reasoning": "Human-readable explanation of why these settings were chosen",
  "scene_type": "portrait",   // Required: scene type for context
  "transition_duration": 1.5, // Required: seconds for smooth transitions

  "exposure": {               // OPTIONAL: core exposure settings
    "mode": "auto",           // "auto", "manual", "aperture_priority", "shutter_priority", "program"
    "iso": 400,              // ISO sensitivity (device-dependent range)
    "shutter_speed": 0.0333, // seconds (e.g., 0.0333 = 1/30s)
    "aperture": 2.8,         // f-stop value
    "ev_bias": 0.0           // -2.0 to +2.0 EV
  },

  "focus": {                  // OPTIONAL: focus settings
    "mode": "auto",           // "auto", "manual", "continuous", "single"
    "distance": 0.5,         // 0.0 (closest) to 1.0 (infinity)
    "point_of_interest": [0.5, 0.4]  // [x, y] coordinates (0.0-1.0)
  },

  "white_balance": {          // OPTIONAL: color temperature
    "preset": "auto",         // "auto", "sunny", "cloudy", "shade", "tungsten", "fluorescent"
    "temperature": 5500,     // Kelvin (2000-10000)
    "tint": 0                // green/magenta adjustment (-50 to +50)
  },

  "processing": {             // OPTIONAL: image processing
    "brightness": 0.0,        // -0.5 to 0.5
    "contrast": 1.0,          // 0.5 to 2.0
    "saturation": 1.0,        // 0.0 to 2.0
    "sharpness": 1.0,         // 0.0 to 2.0
    "filter": "none"          // camera filter name from enum
  },

  "special": {                // OPTIONAL: special modes
    "night_mode": "auto",     // "off", "auto", "on"
    "stabilization": "on",    // "off", "on", "cinematic"
    "zoom": 1.0,             // zoom factor (1.0 = no zoom)
    "flash": "auto",         // "off", "on", "auto"
    "hdr": false,            // High Dynamic Range
    "burst_mode": "off"      // "off", "low", "medium", "high"
  }
}

================================================================================

DEFAULT VALUES (when AI doesn't provide):
- exposure.mode: "auto"
- focus.mode: "auto"
- white_balance.preset: "auto"
- processing.brightness: 0.0
- processing.contrast: 1.0
- processing.saturation: 1.0
- processing.sharpness: 1.0
- processing.filter: "none"
- special.night_mode: "auto"
- special.stabilization: "on"
- special.zoom: 1.0
- special.flash: "auto"
- special.hdr: false
- special.burst_mode: "off"

================================================================================

EXAMPLE MESSAGES:

1. MINIMAL (AI only provides essential settings):
{
  "type": "camera_settings",
  "confidence": 0.78,
  "reasoning": "Bright outdoor conditions detected",
  "scene_type": "landscape",
  "transition_duration": 0.8,
  "exposure": {"mode": "auto"}
}

2. COMPLETE (AI optimizes everything):
{
  "type": "camera_settings",
  "confidence": 0.94,
  "reasoning": "Indoor portrait with tungsten lighting - optimized all settings",
  "scene_type": "portrait",
  "transition_duration": 1.5,
  "exposure": {
    "mode": "manual",
    "iso": 800,
    "shutter_speed": 0.0333,
    "aperture": 2.8,
    "ev_bias": 0.3
  },
  "focus": {
    "mode": "single",
    "distance": 0.4,
    "point_of_interest": [0.5, 0.35]
  },
  "white_balance": {
    "preset": "tungsten",
    "temperature": 2850
  },
  "processing": {
    "brightness": 0.1,
    "contrast": 1.2,
    "saturation": 1.1,
    "sharpness": 0.9,
    "filter": "portrait"
  },
  "special": {
    "night_mode": "auto",
    "stabilization": "on",
    "zoom": 1.2,
    "flash": "off"
  }
}

================================================================================

NOTES FOR BACKEND:
1. All fields except "type", "confidence", "reasoning", "scene_type", "transition_duration" are OPTIONAL
2. AI can provide partial settings - missing ones use device defaults
3. Values must be within device capabilities (we clamp them on iOS side)
4. Send this JSON as part of the WebSocket message payload
5. Use "type": "camera_settings" to identify camera settings messages

================================================================================
*/

// Using the existing PhotoAnalysis and PhotoSuggestion from PoseSuggestionsView
// We need to create WebSocket-specific versions to avoid naming conflicts

// API response models are already defined in PoseSuggestionsView.swift
// Using existing models to avoid naming conflicts

// MARK: - WebSocket Service
class WebSocketService: NSObject, ObservableObject {
    private var webSocketTask: URLSessionWebSocketTask?
    private var urlSession: URLSession?
    private var serverURL: URL {
        let baseURL = "ws://13.221.107.42:4001"
        if let userId = UserDefaults.standard.string(forKey: "database_user_id") {
            return URL(string: "\(baseURL)?userId=\(userId)")!
        } else {
            return URL(string: baseURL)!
        }
    }

    // Published properties for UI updates
    @Published var isConnected = false
    @Published var connectionError: Error?
    @Published var currentRequestId: String?

    // Track analysis completion for proper UI sequencing
    @Published var photoAnalysisCompleted = false

    // Store images when processing UI isn't ready yet
    private var pendingImages: [PoseSuggestion] = []
    private var processingUIReady = false
    private var cardLoadingSequenceStarted = false

    // Event publishers
    let onConnectionEstablished = PassthroughSubject<String, Never>()
    let onProcessingStarted = PassthroughSubject<String, Never>()
    let onAnalysisStarted = PassthroughSubject<String, Never>()
    let onProcessingUIReady = PassthroughSubject<PoseSuggestion, Never>()
    let onShowEmptyCards = PassthroughSubject<Void, Never>() // Signal to show empty shimmer cards
    let onContentAnalysisComplete = PassthroughSubject<CameraSettings, Never>()
    let onImageGenerationStarted = PassthroughSubject<String, Never>()
    let onFirstImageCompleted = PassthroughSubject<PoseSuggestion, Never>()
    let onImageCompleted = PassthroughSubject<PoseSuggestion, Never>()
    let onImageError = PassthroughSubject<(Int, String), Never>()
    let onProcessingCompleted = PassthroughSubject<Void, Never>()
    let onProcessingError = PassthroughSubject<String, Never>()

    private var receivedImages: [PoseSuggestion] = []
    private var expectedTotalImages = 4 // Based on the WebSocket docs
    private var totalMessagesReceived = 0
    private var lastEventTimestamp: Date?

    // Store analysis data from WebSocket events
    private var capturedAnalysis: [PhotoSuggestion] = []

    // MARK: - AI Camera Settings Integration
    // Track applied AI settings for user feedback
    private var lastAppliedAISettings: AICameraRecommendation?
    private var appliedSettingsSummary: [String] = []
    
    // MARK: - Camera Settings Integration
    // Throttling for camera settings to prevent rapid changes
    private var lastCameraSettingsApplied: Date = Date.distantPast
    private let cameraSettingsThrottleInterval: TimeInterval = 2.0
    private weak var cameraService: CameraService?
    private weak var cameraViewModel: CameraViewModel?
    
    // MARK: - Error Handling & Timeout
    private var processingTimeoutTimer: Timer?
    private let processingTimeoutInterval: TimeInterval = 25.0

    // MARK: - Camera Service Integration
    func setCameraService(_ cameraService: CameraService) {
        self.cameraService = cameraService
    }
    
    func setCameraViewModel(_ cameraViewModel: CameraViewModel) {
        self.cameraViewModel = cameraViewModel
    }

    // MARK: - Connection Management
    func connect() {
        // Check if already connected to avoid duplicate connections
        if isConnected && webSocketTask != nil {
            print("ðŸ”Œ WebSocket already connected - reusing existing connection")
            return
        }

        // Clean up any existing connection state
        if webSocketTask != nil {
            print("ðŸ”Œ Cleaning up previous WebSocket connection")
            webSocketTask?.cancel(with: .goingAway, reason: nil)
            webSocketTask = nil
            urlSession = nil
        }

        print("ðŸ”Œ Attempting to connect to WebSocket server at \(serverURL)")
        print("ðŸ”Œ WebSocket URL: \(serverURL.absoluteString)")

        // Try to configure WebSocket with larger buffer sizes (may not work due to system limits)
        let configuration = URLSessionConfiguration.default
        configuration.httpMaximumConnectionsPerHost = 10
        configuration.timeoutIntervalForRequest = 30
        configuration.timeoutIntervalForResource = 300

        // Log system limitations for debugging
        print("ðŸ”Œ iOS WebSocket Limitations:")
        print("ðŸ”Œ - Maximum message size: 1MB (system-imposed)")
        print("ðŸ”Œ - This is a WebKit/iOS system limitation, not configurable via code")
        print("ðŸ”Œ - Large images must use HTTP, small updates use WebSocket")
        print("ðŸ”Œ - Current approach (HTTP + WebSocket) is the recommended solution")

        urlSession = URLSession(configuration: configuration, delegate: self, delegateQueue: nil)
        webSocketTask = urlSession?.webSocketTask(with: serverURL)
        webSocketTask?.resume()

        receiveMessage()

        // Set connected state immediately for optimistic UI updates
        DispatchQueue.main.async {
            self.isConnected = true
            self.connectionError = nil
            print("ðŸ”Œ WebSocket connection initiated - waiting for server confirmation")
        }
    }

    func disconnect() {
        print("ðŸ”Œ Disconnecting from WebSocket server")
        webSocketTask?.cancel(with: .goingAway, reason: nil)
        webSocketTask = nil
        urlSession = nil
        DispatchQueue.main.async {
            self.isConnected = false
            self.receivedImages.removeAll()
        }
    }

    // MARK: - Message Handling
    private func receiveMessage() {
        webSocketTask?.receive { [weak self] result in
            guard let self = self else { return }

            switch result {
            case .success(let message):
                self.handleMessage(message)
                // Continue listening for more messages
                self.receiveMessage()

            case .failure(let error):
                let nsError = error as NSError

                // Handle "Message too long" errors gracefully
                if nsError.domain == NSPOSIXErrorDomain && nsError.code == 40 {
                    print("ðŸ”Œ â„¹ï¸ WebSocket message too large (1MB limit) - expected for image data")
                    print("ðŸ”Œ â„¹ï¸ HTTP fallback will provide the images")
                    // Don't set connection error for expected message size issues
                    return
                }

                print("âŒ WebSocket receive error: \(error)")
                DispatchQueue.main.async {
                    self.connectionError = error
                    self.isConnected = false
                }
            }
        }
    }

    private func handleMessage(_ message: URLSessionWebSocketTask.Message) {
        totalMessagesReceived += 1
        let timestamp = Date()
        lastEventTimestamp = timestamp

        print("ðŸ“¨ [\(totalMessagesReceived)] Received WebSocket message at \(DateFormatter.localizedString(from: timestamp, dateStyle: .none, timeStyle: .medium))")

        switch message {
        case .string(let text):
            print("ðŸ“ Text message (\(text.count) chars): \(text.prefix(200))\(text.count > 200 ? "..." : "")")
            handleTextMessage(text)
        case .data(let data):
            print("ðŸ“¦ Binary data: \(data.count) bytes")
            handleBinaryMessage(data)
        @unknown default:
            print("â“ Unknown message type received")
        }

        // Print summary after each message
        print("ðŸ“Š Response Summary: \(totalMessagesReceived) total messages, \(receivedImages.count) images received")
    }

    private func handleTextMessage(_ text: String) {
        do {
            let event = try JSONDecoder().decode(WebSocketEvent.self, from: Data(text.utf8))
            handleEvent(event)
        } catch {
            print("âŒ Failed to decode WebSocket event: \(error)")
            print("ðŸ“„ Raw message: \(text)")
            print("ðŸ” Message length: \(text.count) characters")

            // Try to parse as generic JSON to see structure
            if let jsonData = text.data(using: .utf8),
               let jsonObject = try? JSONSerialization.jsonObject(with: jsonData, options: []) as? [String: Any] {
                print("ðŸ” JSON structure:")
                for (key, value) in jsonObject {
                    print("   \(key): \(type(of: value)) = \(String(describing: value).prefix(100))")
                }
            }
        }
    }

    private func handleBinaryMessage(_ data: Data) {
        print("ðŸ–¼ï¸ Received binary image data: \(data.count) bytes")

        // Server now sends compressed JPEG data directly as binary buffers
        // We need to convert to base64 for compatibility with existing image handling

        // Check if this looks like a reasonable JPEG image size
        if data.count > 1000 && data.count < 2_000_000 { // 1KB to 2MB range
            let base64String = data.base64EncodedString()
            print("ðŸ”„ Converted binary JPEG to base64: \(base64String.count) chars")

            // Store the binary data as a pending image to be matched with metadata
            storePendingBinaryImage(data: base64String)
            print("ðŸ’¾ Stored compressed JPEG as pending - will match with next image_completed event")

            // Verify it's actually JPEG data by checking header
            if data.count > 4 {
                let header = data.prefix(4).map { String(format: "%02x", $0) }.joined()
                print("ðŸ“‹ Image header: \(header) (should be JPEG: ffd8)")
            }

        } else if data.count > 2_000_000 {
            print("âš ï¸ Binary data too large (\(data.count) bytes) - exceeds 2MB limit")
            print("ðŸ”„ This might be hitting the 1MB WebSocket limit, will rely on HTTP fallback")
        } else {
            print("âš ï¸ Binary data too small (\(data.count) bytes) - might not be an image")
            // Try to interpret as text anyway (could be a malformed message)
            if let text = String(data: data, encoding: .utf8) {
                print("ðŸ“¦ Binary interpreted as text: \(text.prefix(200))\(text.count > 200 ? "..." : "")")
                handleTextMessage(text)
            }
        }
    }

    private var pendingBinaryImages: [String] = []

    private func storePendingBinaryImage(data: String) {
        pendingBinaryImages.append(data)
        print("ðŸ“¦ Pending binary images: \(pendingBinaryImages.count)")
    }

    private func getNextPendingImage() -> String? {
        guard !pendingBinaryImages.isEmpty else { return nil }
        let imageData = pendingBinaryImages.removeFirst()
        print("ðŸ“¦ Retrieved pending binary image, remaining: \(pendingBinaryImages.count)")
        return imageData
    }

    private func handleEvent(_ event: WebSocketEvent) {
        let eventDetails = [
            event.requestId.map { "RequestID: \($0)" },
            event.message.map { "Message: \($0)" },
            event.imageIndex.map { "ImageIndex: \($0)" },
            event.totalImages.map { "TotalImages: \($0)" },
            event.title.map { "Title: \($0)" },
            event.error.map { "Error: \($0)" }
        ].compactMap { $0 }.joined(separator: ", ")

        print("ðŸ“¡ [\(totalMessagesReceived)] Event: \(event.type.rawValue)")
        if !eventDetails.isEmpty {
            print("   ðŸ“‹ Details: \(eventDetails)")
        }

        // Log the current state
        print("   ðŸ“Š Current state: \(getTrackingStatus())")

        // Update current request ID if provided
        if let requestId = event.requestId {
            print("   ðŸ”— Tracking Request ID: \(requestId)")
            DispatchQueue.main.async {
                self.currentRequestId = requestId
            }
        }

        switch event.type {
        case .connection_established:
            if let connectionId = event.requestId {
                print("   ðŸ”Œ âœ… WebSocket connected successfully with ID: \(connectionId)")
                DispatchQueue.main.async {
                    self.onConnectionEstablished.send(connectionId)
                }
            } else {
                print("   ðŸ”Œ âœ… WebSocket connected (no connection ID)")
            }

        case .processing_started:
            if let requestId = event.requestId {
                print("   ðŸš€ âœ… Image processing started - Request: \(requestId)")
                DispatchQueue.main.async {
                    self.onProcessingStarted.send(requestId)
                }
            }

        case .analysis_started:
            if let requestId = event.requestId {
                print("   ðŸ” âœ… AI analysis started - Request: \(requestId)")
                DispatchQueue.main.async {
                    self.onAnalysisStarted.send(requestId)
                }
            }

        case .image_generation_started:
            if let requestId = event.requestId {
                print("   ðŸŽ¨ âœ… Image generation started - Request: \(requestId)")
                DispatchQueue.main.async {
                    self.onImageGenerationStarted.send(requestId)
                }
            } else {
                print("   ðŸš€ âœ… Image processing started")
            }

        case .individual_image_completed:
            print("   ðŸ–¼ï¸ âœ… Image completed - processing...")
            handleImageCompleted(event)

        case .individual_image_error:
            if let imageIndex = event.imageIndex, let error = event.error {
                print("   âŒ Image \(imageIndex) processing failed: \(error)")
                DispatchQueue.main.async {
                    self.onImageError.send((imageIndex, error))
                }
            } else {
                print("   âŒ Image processing error (missing details)")
            }

        case .processing_completed:
            print("   ðŸŽ‰ âœ… All processing completed successfully!")
            print("   ðŸ“Š Final count: \(receivedImages.count) images received")
            clearProcessingTimeout() // Clear timeout on successful completion
            DispatchQueue.main.async {
                self.onProcessingCompleted.send()
            }

        case .processing_error:
            clearProcessingTimeout() // Clear timeout on error
            if let error = event.error {
                print("   ðŸ’¥ âŒ Processing failed with error: \(error)")
                DispatchQueue.main.async {
                    self.onProcessingError.send(error)
                }
            } else {
                print("   ðŸ’¥ âŒ Processing failed (unknown error)")
            }

        case .photo_analysis_complete:
            if let analysis = event.analysis?.photoImprovementSuggestions {
                print("   ðŸ“‹ Photo analysis completed - captured \(analysis.count) improvement suggestions")
                DispatchQueue.main.async {
                    self.capturedAnalysis = analysis
                    self.photoAnalysisCompleted = true  // Mark analysis as complete
                    print("   ðŸ’¾ Analysis data captured and stored for later use with HTTP images")
                    print("   ðŸŽ¯ Analysis phase complete - ready for card loading sequence")

                    // Start card loading sequence immediately after photo analysis completes
                    if !self.cardLoadingSequenceStarted {
                        print("   ðŸŽ¨ Photo analysis complete - starting card loading sequence")
                        self.cardLoadingSequenceStarted = true
                        DispatchQueue.main.asyncAfter(deadline: .now() + 1.5) {
                            print("   ðŸ“¸ Step 1: Show empty shimmer cards with typing animation")
                            self.onShowEmptyCards.send(())

                            // After showing empty cards, wait for typing animation (3 seconds) + image animation (2 seconds) = 5 seconds total
                            DispatchQueue.main.asyncAfter(deadline: .now() + 5.0) {
                                print("   ðŸ“¸ Step 2: Animations complete - showing actual images")
                                self.processingUIReady = true

                                // Send all currently queued images
                                let imagesToSend = self.pendingImages
                                self.pendingImages.removeAll()
                                for queuedImage in imagesToSend {
                                    self.onProcessingUIReady.send(queuedImage)
                                }
                            }
                        }
                    }
                }
            } else {
                print("   ðŸ“‹ Photo analysis completed but no suggestions found")
            }

        case .content_analysis_complete:
            if let settings = event.settings {
                print("   ðŸŽ¯ Content analysis completed - camera settings received")
                clearProcessingTimeout() // Clear timeout on camera settings received
                handleCameraSettingsFromWebSocket(settings)
                DispatchQueue.main.async {
                    self.onContentAnalysisComplete.send(settings)
                }
            } else {
                print("   ðŸŽ¯ Content analysis completed (no camera settings)")
                clearProcessingTimeout() // Clear timeout even if no settings
            }

        case .usage_stats:
            print("   ðŸ“Š Usage stats received")
            if let currentUsage = event.currentUsage,
               let limit = event.limit,
               let plan = event.plan {
                print("   ðŸ“ˆ Current usage: \(currentUsage)/\(limit) (\(plan))")
                if let resetDate = event.resetDate {
                    print("   ðŸ“… Resets on: \(resetDate)")
                }

                // Check if trial user has reached limit and show paywall
                if currentUsage >= limit && plan == "trial" {
                    print("   ðŸŽ¯ Trial user has reached limit (\(currentUsage)/\(limit)) - showing paywall")
                    DispatchQueue.main.async {
                        Superwall.shared.register(placement: "trial_ended")
                        print("   ðŸ’° Superwall paywall shown for trial limit reached")
                    }
                } else {
                    print("   âœ… Usage within limits or not a trial user")
                }
            } else {
                print("   âš ï¸ Usage stats event missing required fields")
            }

        case .plan_limit_reached:
            print("   ðŸš« Plan limit reached")
            if let plan = event.plan,
               let limit = event.limit,
               let resetDate = event.resetDate {
                print("   ðŸ“Š Plan: \(plan), Limit: \(limit), Resets: \(resetDate)")

                // Check if trial user has hit limit mid-session and show paywall
                if plan == "trial" {
                    print("   ðŸŽ¯ Trial user hit limit mid-session - showing paywall")
                    DispatchQueue.main.async {
                        Superwall.shared.register(placement: "trial_ended")
                        print("   ðŸ’° Superwall paywall shown for mid-session trial limit")
                    }
                } else {
                    print("   ðŸ“ˆ Paid user hit limit - would show upgrade paywall (not yet implemented)")
                    // TODO: Show upgrade paywall for paid users
                }
            } else {
                print("   âš ï¸ Plan limit reached event missing required fields")
            }

        default:
            print("   ðŸ“‹ Other event: \(event.type.rawValue) - \(event.message ?? "no message")")
            if let requestId = event.requestId {
                print("   ðŸ”— Associated with request: \(requestId)")
            }
        }

        // Log timing information
        if let lastTimestamp = lastEventTimestamp {
            let timeSinceLast = Date().timeIntervalSince(lastTimestamp)
            print("   â±ï¸ Time since last event: \(String(format: "%.2f", timeSinceLast))s")
        }
    }

    private func handleImageCompleted(_ event: WebSocketEvent) {
        guard let imageIndex = event.imageIndex,
              let title = event.title else {
            print("   âŒ Missing required metadata in image_completed event (imageIndex, title)")
            return
        }

        // Try to get image data from the event (legacy) or pending binary images (new)
        guard let imageData = event.imageData ?? getNextPendingImage() else {
            print("   âŒ No image data available - neither in event nor in pending binary images")
            return
        }

        if event.imageData != nil {
            print("   ðŸ“¦ Using image data from event (legacy format)")
        } else {
            print("   ðŸ“¦ Using image data from pending binary images (new format)")
        }

        let totalImages = event.totalImages ?? 4
        let progress = "\(imageIndex)/\(totalImages)"
        let imageSizeKB = imageData.count / 1024

        print("   ðŸ–¼ï¸ âœ… Image \(progress) completed: '\(title)'")
        print("   ðŸ“Š Image size: \(imageSizeKB)KB, Base64 length: \(imageData.count) chars")

        // Get description from captured analysis data
        let analysisItem = self.capturedAnalysis[safe: imageIndex - 1]
        let description = analysisItem?.poseDescription ?? ""

        // Debug logging for description extraction
        if let analysisItem = analysisItem {
            print("   ðŸ“ Extracted description for image \(imageIndex): '\(description)'")
        } else {
            print("   âš ï¸ No analysis data found for image \(imageIndex) (using empty description)")
        }

        // Create PoseSuggestion from WebSocket data
        let suggestion = PoseSuggestion(
            id: UUID().uuidString,
            image: imageData,
            title: title,
            description: description
        )

        DispatchQueue.main.async {
            self.receivedImages.append(suggestion)

            // Queue all images until processing UI is ready
            print("   ðŸ“¸ Image \(imageIndex) ready - queuing until processing UI is ready")
            self.pendingImages.append(suggestion)

            // Images are queued and will be sent after the card loading sequence completes
            if self.processingUIReady {
                // Processing UI is already ready, send images immediately
                print("   ðŸ“¸ Processing UI ready - sending image immediately")
                self.onImageCompleted.send(suggestion)
                // Remove from pending if it was there
                if let index = self.pendingImages.firstIndex(where: { $0.id == suggestion.id }) {
                    self.pendingImages.remove(at: index)
                }
            }

            // Show progress update
            let currentCount = self.receivedImages.count
            print("   ðŸ“Š Progress: \(currentCount)/\(totalImages) images received")
        }
    }

    // MARK: - Public Methods
    func getCapturedAnalysis() -> [PhotoSuggestion] {
        return capturedAnalysis
    }

    func sendImage(_ image: UIImage, poseSuggestionsEnabled: Bool = false, cameraSettingsEnabled: Bool = false, userId: String? = nil) {
        print("ðŸ“¤ ðŸ“¸ SENDING IMAGE FOR PROCESSING VIA HTTP API...")
        print("ðŸ”Œ WebSocket connection status: \(isConnected ? "Connected" : "Disconnected")")

        // Reset WebSocket state from previous requests
        reset()
        print("ðŸ”„ WebSocket state reset for new image processing")

        // Store parameters for use in completion handlers
        let shouldTrackUsage = poseSuggestionsEnabled
        let currentUserId = userId

        // Check trial usage limit for pose suggestions (parallel with image prep)
        // Temporarily disabled due to API endpoint issues
        let trialCheckTask: Task<Bool, Error>? = nil // shouldTrackUsage && currentUserId != nil ? Task {
        //     guard let userId = currentUserId else { return true }
        //     return try await DatabaseService.shared.checkTrialUsageLimit(for: userId)
        // } : nil

        // Start timeout timer for processing
        startProcessingTimeout()

        // Convert image to JPEG data for HTTP request (optimized for speed)
        guard let imageData = image.jpegData(compressionQuality: 0.7) else {
            print("âŒ Failed to convert image to JPEG")
            return
        }

        // Check trial usage with proper error handling (async, non-blocking)
        if let trialCheckTask = trialCheckTask {
            Task {
                do {
                    let withinLimit = try await trialCheckTask.value
                    if !withinLimit {
                        print("ðŸŽ¯ Trial usage limit reached (5 photos), showing trial_ended paywall")
                        await MainActor.run {
                            Superwall.shared.register(placement: "trial_ended")
                            print("ðŸ’° Superwall paywall shown (note: errors are handled internally by Superwall)")
                        }
                        // Note: Processing may have already started, but user sees paywall
                    } else {
                        print("âœ… Within trial usage limit, proceeding with processing")
                    }
                } catch {
                    print("âŒ Error checking trial usage limit: \(error)")
                    print("â„¹ï¸ Continuing with processing (fail open - don't punish user for our API issues)")
                    // Continue with processing if we can't check the limit (fail open for network issues)
                }
            }
        }

        let imageSizeKB = imageData.count / 1024
        print("ðŸ“¤ Image details: \(imageSizeKB)KB (\(imageData.count) bytes)")
        print("ðŸ”„ Starting HTTP request to trigger WebSocket events...")

        // Create HTTP request to /process-image endpoint with userId as query parameter
        var urlString = "http://13.221.107.42:4000/process-image"
        if let userId = userId {
            urlString += "?userId=\(userId)"
        }
        let httpURL = URL(string: urlString)!
        print("ðŸŒ HTTP Request URL: \(httpURL.absoluteString)")
        var request = URLRequest(url: httpURL)
        request.httpMethod = "POST"

        // Create multipart form data
        let boundary = UUID().uuidString
        request.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")

        var body = Data()

        // Add image data as multipart form field
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"image\"; filename=\"captured-image.jpg\"\r\n".data(using: .utf8)!)
        body.append("Content-Type: image/jpeg\r\n\r\n".data(using: .utf8)!)
        body.append(imageData)
        body.append("\r\n".data(using: .utf8)!)

        // Add pose suggestions enabled parameter
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"poseSuggestionsEnabled\"\r\n\r\n".data(using: .utf8)!)
        body.append("\(poseSuggestionsEnabled)".data(using: .utf8)!)
        body.append("\r\n".data(using: .utf8)!)

        // Add camera settings enabled parameter
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"cameraSettingsEnabled\"\r\n\r\n".data(using: .utf8)!)
        body.append("\(cameraSettingsEnabled)".data(using: .utf8)!)
        body.append("\r\n".data(using: .utf8)!)

        // Add user ID parameter
        if let userId = userId {
            print("ðŸ“‹ Adding userId to HTTP request: \(userId)")
            body.append("--\(boundary)\r\n".data(using: .utf8)!)
            body.append("Content-Disposition: form-data; name=\"userId\"\r\n\r\n".data(using: .utf8)!)
            body.append(userId.data(using: .utf8)!)
            body.append("\r\n".data(using: .utf8)!)
        } else {
            print("âš ï¸ userId is nil - not adding to HTTP request")
        }

        // Add closing boundary
        body.append("--\(boundary)--\r\n".data(using: .utf8)!)

        request.httpBody = body

        print("ðŸ“¤ HTTP request payload size: \(body.count) bytes")
        print("ðŸ”„ Making HTTP POST request to trigger WebSocket events...")

        // Set up a timeout for processing
        DispatchQueue.main.asyncAfter(deadline: .now() + 60) {
            if self.receivedImages.isEmpty {
                print("â° Timeout: No processing events received in 60 seconds")
                print("â„¹ï¸ Possible causes:")
                print("   - HTTP request failed (check server status)")
                print("   - WebSocket not receiving events from HTTP processing")
                print("   - Server processing taking longer than expected")
                print("   - Server might be down or unreachable")
                print("   - WebSocket/HTTP server mismatch")
            }
        }

        // Store the HTTP response data for fallback image retrieval
        var httpResponseData: Data?

        // Make HTTP request
        URLSession.shared.dataTask(with: request) { [shouldTrackUsage, currentUserId] data, response, error in
            DispatchQueue.main.async { [shouldTrackUsage, currentUserId] in
                if let error = error {
                    print("âŒ HTTP request failed: \(error)")
                    return
                }

                if let httpResponse = response as? HTTPURLResponse {
                    print("ðŸ“¡ HTTP Response: \(httpResponse.statusCode)")

                    if (200...299).contains(httpResponse.statusCode) {
                        print("âœ… HTTP request successful - WebSocket should now receive processing events")
                        print("ðŸ”„ Waiting for WebSocket events: processing_started, individual_image_completed, etc.")

                        // Store response data for fallback
                        httpResponseData = data

                        // Try to parse HTTP response for image data
                        if let responseData = data {
                            self.parseHTTPResponseData(responseData, shouldTrackUsage: shouldTrackUsage, currentUserId: currentUserId)
                        }
                    } else {
                        print("âŒ HTTP request failed with status: \(httpResponse.statusCode)")
                        if let data = data, let errorString = String(data: data, encoding: .utf8) {
                            print("ðŸ“„ Error response: \(errorString)")
                        }
                    }
                } else {
                    print("âŒ Invalid HTTP response")
                }
            }
        }.resume()
    }

    func getReceivedImages() -> [PoseSuggestion] {
        return receivedImages
    }

    func getTrackingStatus() -> String {
        let connectionStatus = isConnected ? "Connected" : "Disconnected"
        let imagesReceived = receivedImages.count
        let messagesReceived = totalMessagesReceived
        let lastActivity = lastEventTimestamp.map { "Last: \(DateFormatter.localizedString(from: $0, dateStyle: .none, timeStyle: .medium))" } ?? "No activity"

        return "Status: \(connectionStatus) | Images: \(imagesReceived)/4 | Messages: \(messagesReceived) | \(lastActivity)"
    }

    private func parseHTTPResponseData(_ data: Data, shouldTrackUsage: Bool, currentUserId: String?) {
        do {
            // Use the JSONDecoder with the correct key decoding strategy for snake_case
            let decoder = JSONDecoder()
            decoder.keyDecodingStrategy = .convertFromSnakeCase

            let apiResponse = try decoder.decode(ApiResponse.self, from: data)
            print("ðŸ“„ HTTP Response parsed successfully")
            print("ðŸ“„ Found \(apiResponse.generatedImages.count) images in HTTP response")

            // Convert API response to PoseSuggestion objects
            let suggestions = apiResponse.generatedImages.enumerated().map { index, image in
                // Create a GeneratedImage instance for the PoseSuggestion initializer
                let generatedImage = GeneratedImage(data: image.data, mimeType: image.mimeType)

                // Use captured analysis from WebSocket if available, otherwise try HTTP response
                let analysis = self.capturedAnalysis[safe: index] ?? apiResponse.analysis?.photoImprovementSuggestions?[safe: index]

                // Debug: Log what analysis data we're getting
                if let analysis = analysis {
                    print("ðŸ“„ Image \(index + 1) analysis: title='\(analysis.title ?? "nil")', description='\(analysis.poseDescription ?? "nil")' (source: \(self.capturedAnalysis[safe: index] != nil ? "WebSocket" : "HTTP"))")
                } else {
                    print("ðŸ“„ Image \(index + 1) analysis: nil (will use default title)")
                }

                return PoseSuggestion(
                    from: generatedImage,
                    index: index,
                    analysis: analysis
                )
            }

            print("ðŸ“„ Created \(suggestions.count) pose suggestions from HTTP response")

            // If we don't have images from WebSocket yet, use HTTP response as fallback
            // But only if WebSocket hasn't started sending events yet
            if self.receivedImages.isEmpty && !suggestions.isEmpty && self.totalMessagesReceived < 10 {
                print("ðŸ”„ Using HTTP response as fallback for image data (WebSocket inactive)")
                DispatchQueue.main.async {
                    self.receivedImages = suggestions

                    // Send first image completed event
                    if let firstSuggestion = suggestions.first {
                        print("ðŸŽ¯ FIRST IMAGE READY (from HTTP) - Switching to preview overlay")
                        self.onFirstImageCompleted.send(firstSuggestion)
                    }

                    // Send remaining images
                    for (index, suggestion) in suggestions.enumerated() where index > 0 {
                        print("ðŸ“¸ Additional image \(index + 1) ready (from HTTP) - Adding to overlay")
                        self.onImageCompleted.send(suggestion)
                    }

                    // Increment trial usage if pose suggestions were enabled
                    if shouldTrackUsage, let userId = currentUserId {
                        Task {
                            do {
                                try await DatabaseService.shared.incrementTrialUsage(for: userId)
                                print("ðŸ“Š Trial usage incremented for user \(userId)")
                            } catch {
                                print("âŒ Failed to increment trial usage: \(error)")
                            }
                        }
                    }

                    // Send processing completed
                    print("ðŸŽ‰ All processing completed (from HTTP)")
                    self.onProcessingCompleted.send()
                }
            }

        } catch {
            print("âŒ Failed to parse HTTP response: \(error)")
            let rawResponse = String(data: data, encoding: .utf8) ?? "Unable to decode"
            print("ðŸ“„ Raw HTTP response (first 2000 chars): \(rawResponse.prefix(2000))")

            // Try to debug the structure
            if rawResponse.contains("analysis") {
                print("ðŸ“„ HTTP response DOES contain 'analysis' field")
                // Try to extract analysis section
                if let analysisStart = rawResponse.range(of: "\"analysis\":"),
                   let analysisEnd = rawResponse.range(of: "\"generated_images\":", range: analysisStart.upperBound..<rawResponse.endIndex) {
                    let analysisSection = rawResponse[analysisStart.lowerBound..<analysisEnd.lowerBound]
                    print("ðŸ“„ Analysis section: \(analysisSection.prefix(500))")
                }
            } else {
                print("ðŸ“„ HTTP response does NOT contain 'analysis' field")
            }

            if rawResponse.contains("photo_improvement_suggestions") {
                print("ðŸ“„ HTTP response DOES contain 'photo_improvement_suggestions'")
            } else {
                print("ðŸ“„ HTTP response does NOT contain 'photo_improvement_suggestions'")
            }

            // Try to manually parse with different structure
            print("ðŸ” Attempting manual JSON parsing...")
            if let jsonObject = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any] {
                print("ðŸ“‹ JSON keys found: \(jsonObject.keys.joined(separator: ", "))")

                if let analysis = jsonObject["analysis"] as? [String: Any] {
                    print("ðŸ“‹ Analysis object found with keys: \(analysis.keys.joined(separator: ", "))")

                    if let suggestions = analysis["photo_improvement_suggestions"] as? [[String: Any]] {
                        print("ðŸ“‹ Found \(suggestions.count) photo improvement suggestions")
                        for (i, suggestion) in suggestions.enumerated() {
                            print("ðŸ“‹ Suggestion \(i+1): \(suggestion.keys.joined(separator: ", ")) - Title: \(suggestion["title"] ?? "nil")")
                        }
                    }
                } else {
                    print("ðŸ“‹ No analysis object found")
                }
            }
        }
    }

    func reset() {
        DispatchQueue.main.async {
            self.receivedImages.removeAll()
            self.capturedAnalysis.removeAll()
            self.pendingBinaryImages.removeAll()
            self.currentRequestId = nil
            self.connectionError = nil
            self.totalMessagesReceived = 0
            self.lastEventTimestamp = nil
            self.photoAnalysisCompleted = false  // Reset analysis completion flag
            self.pendingImages.removeAll()  // Clear any pending images
            self.processingUIReady = false  // Reset processing UI state
            self.cardLoadingSequenceStarted = false  // Reset card loading sequence flag
        }
        print("ðŸ”„ WebSocket tracking reset - cleared images, analysis, and pending binary data")
    }
}

// MARK: - URLSessionWebSocketDelegate
extension WebSocketService: URLSessionWebSocketDelegate {
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocolName: String?) {
        print("ðŸ”Œ âœ… WebSocket connection successfully opened")
        print("ðŸ”Œ â„¹ï¸ Connection established - ready for low-latency image processing")
        if let protocolValue = protocolName {
            print("ðŸ”Œ Protocol: \(protocolValue)")
        }
        DispatchQueue.main.async {
            self.isConnected = true
            self.connectionError = nil
            print("ðŸ”Œ Connection state updated to: Connected")
        }
    }

    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        print("ðŸ”Œ âŒ WebSocket connection closed with code: \(closeCode.rawValue)")
        if let reason = reason, let reasonString = String(data: reason, encoding: .utf8) {
            print("ðŸ”Œ Close reason: \(reasonString)")
        }
        DispatchQueue.main.async {
            self.isConnected = false
            print("ðŸ”Œ Connection state updated to: Disconnected")
        }
    }

    func urlSession(_ session: URLSession, task: URLSessionTask, didCompleteWithError error: Error?) {
        if let error = error {
            let nsError = error as NSError

            // Handle "Message too long" errors more gracefully
            if nsError.domain == NSPOSIXErrorDomain && nsError.code == 40 {
                print("ðŸ”Œ â„¹ï¸ WebSocket message size limit exceeded (1MB) - this is expected")
                print("ðŸ”Œ â„¹ï¸ Large images are handled via HTTP, WebSocket handles progress updates")
                print("ðŸ”Œ â„¹ï¸ This is the recommended iOS WebSocket pattern")
                return // Don't set connection error for expected message size issues
            }

            print("ðŸ”Œ âŒ WebSocket task failed: \(error.localizedDescription)")
            print("ðŸ”Œ Error domain: \(nsError.domain), code: \(nsError.code)")
            DispatchQueue.main.async {
                self.isConnected = false
                self.connectionError = error
                print("ðŸ”Œ Connection state updated to: Error")
            }
        } else {
            print("ðŸ”Œ âœ… WebSocket task completed successfully")
        }
    }

    // MARK: - AI Camera Settings Integration

    /// Applies AI-recommended camera settings to the camera service
    /// - Parameter recommendation: The AI camera settings recommendation
    /// - Parameter cameraService: The camera service to apply settings to
    func applyAICameraRecommendation(_ recommendation: AICameraRecommendation, to cameraService: CameraService) {
        print("ðŸ¤– Applying AI camera recommendation (confidence: \(recommendation.confidence))")
        print("ðŸ“ Reasoning: \(recommendation.reasoning)")
        print("ðŸŽ­ Scene type: \(recommendation.sceneType)")

        // Store for user feedback
        lastAppliedAISettings = recommendation
        appliedSettingsSummary.removeAll()

        // Apply settings with smooth transitions
        let transitionDuration = recommendation.transitionDuration

        // Apply exposure settings
        if let exposure = recommendation.exposure {
            applyExposureSettings(exposure, to: cameraService, transitionDuration: transitionDuration)
        }

        // Apply focus settings
        if let focus = recommendation.focus {
            applyFocusSettings(focus, to: cameraService, transitionDuration: transitionDuration)
        }

        // Apply white balance settings
        if let whiteBalance = recommendation.whiteBalance {
            applyWhiteBalanceSettings(whiteBalance, to: cameraService, transitionDuration: transitionDuration)
        }

        // Apply processing settings
        if let processing = recommendation.processing {
            applyProcessingSettings(processing, to: cameraService, transitionDuration: transitionDuration)
        }

        // Apply special settings
        if let special = recommendation.special {
            applySpecialSettings(special, to: cameraService, transitionDuration: transitionDuration)
        }

        print("âœ… AI camera settings applied successfully")
        print("ðŸ“Š Applied \(appliedSettingsSummary.count) setting changes")
    }

    private func applyExposureSettings(_ exposure: AIExposureSettings, to cameraService: CameraService, transitionDuration: TimeInterval) {
        if let mode = exposure.mode {
            switch mode {
            case "auto":
                cameraService.exposureMode = .auto
                appliedSettingsSummary.append("ðŸ“· Exposure mode: Auto")
            case "manual":
                cameraService.exposureMode = .manual
                appliedSettingsSummary.append("ðŸ“· Exposure mode: Manual")
            case "aperture_priority":
                cameraService.exposureMode = .priorityAperture
                appliedSettingsSummary.append("ðŸ“· Exposure mode: Aperture Priority")
            case "shutter_priority":
                cameraService.exposureMode = .priorityShutter
                appliedSettingsSummary.append("ðŸ“· Exposure mode: Shutter Priority")
            case "program":
                cameraService.exposureMode = .program
                appliedSettingsSummary.append("ðŸ“· Exposure mode: Program")
            default:
                break
            }
        }

        if let iso = exposure.iso {
            appliedSettingsSummary.append("ðŸ”† ISO: \(Int(iso))")
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.2) { [weak cameraService] in
                cameraService?.setISO(CGFloat(iso))
            }
        }

        if let shutterSpeed = exposure.shutterSpeed {
            let shutterFraction = Int(1.0 / shutterSpeed)
            appliedSettingsSummary.append("âš¡ Shutter: 1/\(shutterFraction)s")
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.4) { [weak cameraService] in
                cameraService?.setShutterSpeed(CMTime(seconds: Double(shutterSpeed), preferredTimescale: 1000000))
            }
        }

        if let aperture = exposure.aperture {
            appliedSettingsSummary.append("ðŸ”µ Aperture: f/\(aperture)")
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.6) { [weak cameraService] in
                cameraService?.aperture = CGFloat(aperture)
            }
        }

        if let evBias = exposure.evBias {
            appliedSettingsSummary.append("ðŸ“Š EV Bias: \(evBias > 0 ? "+" : "")\(evBias)EV")
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.8) { [weak cameraService] in
                cameraService?.setExposureBias(CGFloat(evBias))
            }
        }
    }

    private func applyFocusSettings(_ focus: AIFocusSettings, to cameraService: CameraService, transitionDuration: TimeInterval) {
        if let mode = focus.mode {
            switch mode {
            case "auto":
                cameraService.focusMode = .auto
            case "manual":
                cameraService.focusMode = .manual
            case "continuous":
                cameraService.focusMode = .continuous
            case "single":
                cameraService.focusMode = .single
            default:
                break
            }
        }

        if let distance = focus.distance {
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.3) { [weak cameraService] in
                cameraService?.setFocusDistance(CGFloat(distance))
            }
        }

        if let pointOfInterest = focus.pointOfInterest, pointOfInterest.count >= 2 {
            let point = CGPoint(x: CGFloat(pointOfInterest[0]), y: CGFloat(pointOfInterest[1]))
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.5) { [weak cameraService] in
                cameraService?.focus(at: point)
            }
        }
    }

    private func applyWhiteBalanceSettings(_ wb: AIWhiteBalanceSettings, to cameraService: CameraService, transitionDuration: TimeInterval) {
        if let preset = wb.preset {
            switch preset {
            case "auto":
                cameraService.wbPreset = .auto
            case "sunny":
                cameraService.wbPreset = .sunny
            case "cloudy":
                cameraService.wbPreset = .cloudy
            case "shade":
                cameraService.wbPreset = .shade
            case "tungsten":
                cameraService.wbPreset = .tungsten
            case "fluorescent":
                cameraService.wbPreset = .fluorescent
            default:
                break
            }
            cameraService.applyWhiteBalancePreset(cameraService.wbPreset)
        }

        if let temperature = wb.temperature {
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.7) { [weak cameraService] in
                cameraService?.setWhiteBalanceTemperature(CGFloat(temperature), tint: wb.tint != nil ? CGFloat(wb.tint!) : 0.0)
            }
        }
    }

    private func applyProcessingSettings(_ processing: AIProcessingSettings, to cameraService: CameraService, transitionDuration: TimeInterval) {
        if let brightness = processing.brightness {
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.2) { [weak cameraService] in
                cameraService?.setBrightness(CGFloat(brightness))
            }
        }

        if let contrast = processing.contrast {
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.4) { [weak cameraService] in
                cameraService?.setContrast(CGFloat(contrast))
            }
        }

        if let saturation = processing.saturation {
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.6) { [weak cameraService] in
                cameraService?.setSaturation(CGFloat(saturation))
            }
        }

        if let sharpness = processing.sharpness {
            cameraService.sharpness = CGFloat(sharpness)
        }

        if let filter = processing.filter {
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.8) { [weak cameraService] in
                if let cameraFilter = CameraFilter(rawValue: filter) {
                    cameraService?.setFilter(cameraFilter)
                }
            }
        }
    }

    private func applySpecialSettings(_ special: AISpecialSettings, to cameraService: CameraService, transitionDuration: TimeInterval) {
        if let nightMode = special.nightMode {
            switch nightMode {
            case "off":
                cameraService.nightMode = .off
            case "auto":
                cameraService.nightMode = .auto
            case "on":
                cameraService.nightMode = .on
            default:
                break
            }
            cameraService.setNightMode(cameraService.nightMode)
        }

        if let stabilization = special.stabilization {
            switch stabilization {
            case "off":
                cameraService.stabilizationMode = .off
            case "on":
                cameraService.stabilizationMode = .on
            case "cinematic":
                cameraService.stabilizationMode = .cinematic
            default:
                break
            }
        }

        if let zoom = special.zoom {
            DispatchQueue.main.asyncAfter(deadline: .now() + transitionDuration * 0.5) { [weak cameraService] in
                cameraService?.setZoom(factor: CGFloat(zoom))
            }
        }

        if let flash = special.flash {
            switch flash {
            case "off":
                cameraService.flashMode = .off
            case "on":
                cameraService.flashMode = .on
            case "auto":
                cameraService.flashMode = .auto
            default:
                break
            }
        }

        if let hdr = special.hdr {
            cameraService.hdrMode = hdr
        }

        if let burstMode = special.burstMode {
            switch burstMode {
            case "off":
                cameraService.burstMode = .off
            case "low":
                cameraService.burstMode = .low
            case "medium":
                cameraService.burstMode = .medium
            case "high":
                cameraService.burstMode = .high
            default:
                break
            }
        }
    }

    // MARK: - Processing Timeout Handling
    
    private func startProcessingTimeout() {
        // Clear any existing timer
        processingTimeoutTimer?.invalidate()
        
        // Start new timeout timer
        processingTimeoutTimer = Timer.scheduledTimer(withTimeInterval: processingTimeoutInterval, repeats: false) { [weak self] _ in
            self?.handleProcessingTimeout()
        }
        
        print("â° Started processing timeout timer for \(processingTimeoutInterval) seconds")
    }
    
    private func clearProcessingTimeout() {
        processingTimeoutTimer?.invalidate()
        processingTimeoutTimer = nil
        print("âœ… Cleared processing timeout timer")
    }
    
    private func handleProcessingTimeout() {
        print("â° Processing timeout reached after \(processingTimeoutInterval) seconds")
        
        DispatchQueue.main.async {
            if let viewModel = self.cameraViewModel {
                viewModel.showProcessingTimeout()
            }
        }
        
        // Clear the timer
        processingTimeoutTimer = nil
    }

    // MARK: - Camera Settings from WebSocket
    
    private func handleCameraSettingsFromWebSocket(_ settings: CameraSettings) {
        // Check throttling to prevent rapid setting changes
        let now = Date()
        guard now.timeIntervalSince(lastCameraSettingsApplied) >= cameraSettingsThrottleInterval else {
            print("   â±ï¸ Camera settings throttled - ignoring (last applied \(String(format: "%.1f", now.timeIntervalSince(lastCameraSettingsApplied)))s ago)")
            return
        }
        
        guard let cameraService = cameraService else {
            print("   âŒ Camera service not available for applying settings")
            return
        }
        
        // Apply settings on background thread to prevent FPS drops
        DispatchQueue.global(qos: .userInitiated).async {
            let appliedSettings = cameraService.applyCameraSettingsFromWebSocket(settings)
            
            DispatchQueue.main.async {
                if appliedSettings.isEmpty {
                    print("   â„¹ï¸ No camera settings were applied")
                } else {
                    print("   âœ… Applied \(appliedSettings.count) camera settings:")
                    for setting in appliedSettings {
                        print("     - \(setting)")
                    }
                }
                
                // Hide camera settings overlay if it's showing
                if let viewModel = self.cameraViewModel {
                    viewModel.hideCameraSettingsOverlay()
                }
                
                // Update throttling timestamp
                self.lastCameraSettingsApplied = now
            }
        }
    }

    // MARK: - AI Settings Management

    /// Get information about currently applied AI settings
    func getAppliedAISettingsInfo() -> [String] {
        if appliedSettingsSummary.isEmpty {
            return ["ðŸ¤– AI settings: None applied yet"]
        }

        var summary = ["ðŸ¤– AI Applied Settings:"]
        summary.append(contentsOf: appliedSettingsSummary)

        if let lastSettings = lastAppliedAISettings {
            summary.append("ðŸ“ Reasoning: \(lastSettings.reasoning)")
            summary.append("ðŸŽ¯ Confidence: \(Int(lastSettings.confidence * 100))%")
        }

        return summary
    }


}
